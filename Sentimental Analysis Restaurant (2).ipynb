{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f3a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6cef12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1\n",
       "5     Now I am getting angry and I want my damn pho.      0\n",
       "6              Honeslty it didn't taste THAT fresh.)      0\n",
       "7  The potatoes were like rubber and you could te...      0\n",
       "8                          The fries were great too.      1\n",
       "9                                     A great touch.      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"Restaurant_Reviews.tsv\"\n",
    "dataset = pd.read_csv(file_path, delimiter='\\t', quoting=3)\n",
    "dataset.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586ef23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf356d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "vectorizer = CountVectorizer(max_features=1500)\n",
    "X = vectorizer.fit_transform(dataset['Review']).toarray()\n",
    "y = dataset['Liked']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b0a9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82        96\n",
      "           1       0.87      0.76      0.81       104\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.82      0.82      0.81       200\n",
      "weighted avg       0.82      0.81      0.81       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35d3c2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79        96\n",
      "           1       0.83      0.75      0.79       104\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.79      0.79      0.79       200\n",
      "weighted avg       0.79      0.79      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6459c3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.85      0.80        96\n",
      "           1       0.84      0.73      0.78       104\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.79      0.79      0.79       200\n",
      "weighted avg       0.80      0.79      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train model\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "979488b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.89      0.78        96\n",
      "           1       0.86      0.64      0.74       104\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.78      0.76      0.76       200\n",
      "weighted avg       0.78      0.76      0.76       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb4403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f75baa83",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train model\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ea737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative Machine Learning Models for Sentiment Analysis\n",
    "#Since Multinomial Naïve Bayes (MultinomialNB) is used, exploring other models can improve accuracy and performance.\n",
    "\n",
    "#Logistic Regression: A strong baseline for text classification. It is simple, interpretable, and effective for binary sentiment analysis but struggles with highly imbalanced datasets.\n",
    "\n",
    "#Support Vector Machine (SVM): Works well with high-dimensional text data. It is powerful for small datasets and captures decision boundaries effectively but can be computationally expensive.\n",
    "\n",
    "#Random Forest: A robust ensemble learning method that handles complex relationships between words well. It is less prone to overfitting compared to Naïve Bayes but may be slower with large feature sets.\n",
    "\n",
    "#XGBoost: A boosting algorithm designed for structured text data. It provides high accuracy and handles overfitting well but requires tuning for optimal performance.\n",
    "\n",
    "#Deep Learning Models (LSTM/BERT): These models understand word relationships and contextual meaning in sentiment analysis. They perform exceptionally well but require more data and computational resources.\n",
    "\n",
    "#Choosing the Right Model\n",
    "#If you need simplicity and speed, go for Logistic Regression.\n",
    "\n",
    "#If you want higher accuracy, SVM is a great choice.\n",
    "\n",
    "#If your dataset is large, XGBoost is optimal.\n",
    "\n",
    "#If you need interpretability, Random Forest is a good option.\n",
    "\n",
    "#If you want deep contextual understanding, LSTM/BERT will provide the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a43c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apart from vectorizer which method can be used\n",
    "\n",
    "\n",
    "# There are several alternative methods to CountVectorizer for converting text into numerical form for machine learning models. Each has its own strengths and is suited to different use cases. Here's a detailed comparison:\n",
    "\n",
    "# 🔹 1. TfidfVectorizer (Term Frequency - Inverse Document Frequency)\n",
    "# 📌 Description:\n",
    "# Unlike CountVectorizer, which counts word frequency, TfidfVectorizer measures how important a word is in a document relative to the entire corpus.\n",
    "\n",
    "# It reduces the impact of frequently occurring but less meaningful words (like \"the\", \"is\").\n",
    "\n",
    "# ✅ Pros:\n",
    "# Gives less weight to common words, more to unique ones.\n",
    "\n",
    "# Improves performance in many NLP tasks.\n",
    "\n",
    "# 📦 Code Example:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorizer = TfidfVectorizer(max_features=1500)\n",
    "# X = vectorizer.fit_transform(dataset['Review']).toarray()\n",
    "# 🔹 2. HashingVectorizer\n",
    "# 📌 Description:\n",
    "# Uses a hashing trick to convert words to numbers without storing the vocabulary.\n",
    "\n",
    "# Fixed-size vector space; very fast and memory-efficient.\n",
    "\n",
    "# ✅ Pros:\n",
    "# Good for large-scale applications and online learning.\n",
    "\n",
    "# No need to keep a vocabulary dictionary.\n",
    "\n",
    "# ❌ Cons:\n",
    "# Cannot reverse-map numbers back to words.\n",
    "\n",
    "# Possible hash collisions.\n",
    "\n",
    "# 📦 Code Example:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# vectorizer = HashingVectorizer(n_features=1500)\n",
    "# X = vectorizer.transform(dataset['Review']).toarray()\n",
    "# 🔹 3. Word Embeddings (Word2Vec / GloVe / FastText)\n",
    "# 📌 Description:\n",
    "# Word embeddings represent words as dense vectors (e.g., 100 or 300 dimensions) capturing semantic meaning.\n",
    "\n",
    "# Pre-trained embeddings like GloVe or Word2Vec are often used.\n",
    "\n",
    "# ✅ Pros:\n",
    "# Captures context and meaning (e.g., “king” - “man” + “woman” ≈ “queen”).\n",
    "\n",
    "# Useful for deep learning and NLP tasks.\n",
    "\n",
    "# ❌ Cons:\n",
    "# More complex to implement.\n",
    "\n",
    "# Requires more compute power.\n",
    "\n",
    "# 📦 Code Example (with Gensim Word2Vec):\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from gensim.models import Word2Vec\n",
    "# sentences = [review.split() for review in dataset['Review']]\n",
    "# model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n",
    "# Or you can use pre-trained GloVe/Word2Vec and average word vectors for each review.\n",
    "\n",
    "# 🔹 4. Transformers / BERT Embeddings\n",
    "# 📌 Description:\n",
    "# State-of-the-art models like BERT, RoBERTa, etc., generate contextual embeddings.\n",
    "\n",
    "# Words are embedded differently based on their surrounding words.\n",
    "\n",
    "# ✅ Pros:\n",
    "# Extremely powerful, captures deep context.\n",
    "\n",
    "# Great for sentiment analysis and many NLP tasks.\n",
    "\n",
    "# ❌ Cons:\n",
    "# Requires GPU or strong CPU.\n",
    "\n",
    "# More complex to implement.\n",
    "\n",
    "# 📦 Example with transformers:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# import torch\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# inputs = tokenizer(\"Sample review text\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "# ✅ Summary Table:\n",
    "# Method\tContext-Aware?\tSparse/Dense\tEasy to Use\tBest For\n",
    "# CountVectorizer\t❌ No\tSparse\t✅ Yes\tBasic text classification\n",
    "# TfidfVectorizer\t❌ No\tSparse\t✅ Yes\tBetter weighted word importance\n",
    "# HashingVectorizer\t❌ No\tSparse\t✅ Yes\tLarge datasets, fast processing\n",
    "# Word2Vec / GloVe\t✅ Yes (some)\tDense\t⚠ Medium\tSemantics and similarity\n",
    "# BERT / Transformers\t✅✅ Very Much\tDense\t❌ Complex\tAdvanced NLP tasks\n",
    "# Let me know your use case (e.g., sentiment analysis, spam detection, chatbot) — I can suggest the best method with example code!\n",
    "\n",
    "\n",
    "\n",
    "# ✅ By default, CountVectorizer does convert text to lowercase automatically.\n",
    "# So you don’t have to explicitly convert text to lowercase before using it.\n",
    "\n",
    "# 🔍 Here's what happens under the hood:\n",
    "# When you use:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# vectorizer = CountVectorizer()\n",
    "# Internally, it uses:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# preprocessor = text.lower()\n",
    "# This is controlled by the parameter:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# lowercase=True  # default behavior\n",
    "# 🛠️ If you want to disable it for any reason:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# vectorizer = CountVectorizer(lowercase=False)\n",
    "# This will preserve original casing (e.g., \"Apple\" and \"apple\" will be treated as different words).\n",
    "\n",
    "# 🧪 Quick Example:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# docs = [\"I Love NLP\", \"i love nlp\"]\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(docs)\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "# 📤 Output:\n",
    "\n",
    "# css\n",
    "# Copy\n",
    "# Edit\n",
    "# ['love' 'nlp']\n",
    "# So both \"Love\" and \"love\" are treated the same due to automatic lowercasing.\n",
    "\n",
    "# 💡 Tip:\n",
    "# Even though CountVectorizer lowers the case, you might still want to do other cleaning steps manually before vectorizing:\n",
    "\n",
    "# Removing punctuation\n",
    "\n",
    "# Removing stopwords\n",
    "\n",
    "# Lemmatization or stemming\n",
    "\n",
    "# This is especially useful when you want more control over:\n",
    "\n",
    "# Cleaning your text\n",
    "\n",
    "# Improving model performance\n",
    "\n",
    "# Reducing noise\n",
    "\n",
    "# 🛠️ NLP Text Preprocessing Pipeline – Step-by-Step\n",
    "# We'll cover each of these:\n",
    "\n",
    "# Lowercasing\n",
    "\n",
    "# Removing punctuation\n",
    "\n",
    "# Removing stopwords\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "# Stemming or Lemmatization\n",
    "\n",
    "# Rejoining tokens\n",
    "\n",
    "# We’ll use nltk and re libraries here.\n",
    "\n",
    "# 🔹 1. Lowercasing\n",
    "# 📌 Purpose:\n",
    "# To make the text uniform — \"Apple\" and \"apple\" will be treated the same.\n",
    "\n",
    "# ✅ Code:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# text = \"I Love NLP!\"\n",
    "# text = text.lower()\n",
    "# print(text)  # Output: \"i love nlp!\"\n",
    "# 🔹 2. Removing Punctuation\n",
    "# 📌 Purpose:\n",
    "# Punctuation is usually not useful for sentiment analysis, classification, etc.\n",
    "\n",
    "# ✅ Code:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# import re\n",
    "\n",
    "# text = re.sub(r'[^\\w\\s]', '', text)\n",
    "# print(text)  # Output: \"i love nlp\"\n",
    "# \\w matches words (letters/numbers)\n",
    "\n",
    "# \\s matches spaces\n",
    "\n",
    "# ^ negates — so it removes anything that is NOT a word or space\n",
    "\n",
    "# 🔹 3. Removing Stopwords\n",
    "# 📌 Purpose:\n",
    "# Stopwords like \"is\", \"the\", \"and\", etc., appear very frequently and usually don't carry meaning.\n",
    "\n",
    "# ✅ Code:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# words = text.split()\n",
    "# filtered_words = [word for word in words if word not in stop_words]\n",
    "# print(filtered_words)  # Output: ['love', 'nlp']\n",
    "# 🔹 4. Tokenization\n",
    "# 📌 Purpose:\n",
    "# Splitting a sentence into individual words (tokens).\n",
    "\n",
    "# ✅ Code (basic):\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# text = \"I love NLP\"\n",
    "# tokens = word_tokenize(text.lower())\n",
    "# print(tokens)  # Output: ['i', 'love', 'nlp']\n",
    "# 🔹 5. Stemming or Lemmatization\n",
    "# You can choose one:\n",
    "\n",
    "# 🟣 5A. Stemming (Porter Stemmer)\n",
    "# Cuts off word suffixes.\n",
    "\n",
    "# Less accurate, but faster.\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from nltk.stem import PorterStemmer\n",
    "# ps = PorterStemmer()\n",
    "\n",
    "# stemmed = [ps.stem(word) for word in filtered_words]\n",
    "# print(stemmed)  # Output: ['love', 'nlp'] (no change in this example)\n",
    "# 🟢 5B. Lemmatization (WordNet Lemmatizer)\n",
    "# Smarter – returns actual base/root word (lemma).\n",
    "\n",
    "# Slower but more accurate.\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "# print(lemmatized)  # Output: ['love', 'nlp']\n",
    "# 🔹 6. Rejoining Tokens\n",
    "# 📌 Purpose:\n",
    "# After preprocessing, you might want to convert tokens back into full cleaned-up text.\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# cleaned_text = ' '.join(lemmatized)\n",
    "# print(cleaned_text)  # Output: \"love nlp\"\n",
    "# ✅ Full Pipeline Function\n",
    "# Here’s everything in one function:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Downloads (run once)\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Lowercase\n",
    "#     text = text.lower()\n",
    "    \n",
    "#     # Remove punctuation\n",
    "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "#     # Tokenize\n",
    "#     tokens = word_tokenize(text)\n",
    "    \n",
    "#     # Remove stopwords\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "#     # Lemmatize\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "#     # Rejoin\n",
    "#     cleaned_text = ' '.join(tokens)\n",
    "#     return cleaned_text\n",
    "# 🔄 Usage Example:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# dataset['Cleaned_Review'] = dataset['Review'].apply(preprocess_text)\n",
    "# Then you can vectorize:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# vectorizer = CountVectorizer(max_features=1500)\n",
    "# X = vectorizer.fit_transform(dataset['Cleaned_Review']).toarray()\n",
    "\n",
    "# Let's now create the same custom text preprocessing pipeline using spaCy, which is a powerful and modern NLP library.\n",
    "\n",
    "# 🚀 Why spaCy?\n",
    "# It's faster and more accurate than NLTK.\n",
    "\n",
    "# Automatically handles:\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "# Lowercasing\n",
    "\n",
    "# Lemmatization\n",
    "\n",
    "# POS tagging (if needed)\n",
    "\n",
    "# Named Entity Recognition\n",
    "\n",
    "# Offers cleaner and more efficient code.\n",
    "\n",
    "# 🧪 Installation (if needed):\n",
    "# bash\n",
    "# Copy\n",
    "# Edit\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# 🛠️ spaCy Preprocessing Pipeline\n",
    "# Here’s a full function using spaCy for:\n",
    "\n",
    "# Lowercasing\n",
    "\n",
    "# Removing punctuation\n",
    "\n",
    "# Removing stopwords\n",
    "\n",
    "# Lemmatization\n",
    "\n",
    "# Rejoining tokens\n",
    "\n",
    "# ✅ Code:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# import spacy\n",
    "\n",
    "# # Load spaCy English model\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# def spacy_preprocess(text):\n",
    "#     doc = nlp(text)\n",
    "#     tokens = []\n",
    "\n",
    "#     for token in doc:\n",
    "#         # Filter out stopwords and punctuation\n",
    "#         if not token.is_stop and not token.is_punct:\n",
    "#             # Lemmatize and lowercase\n",
    "#             tokens.append(token.lemma_.lower())\n",
    "\n",
    "#     return ' '.join(tokens)\n",
    "# 🔄 Example:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# text = \"I absolutely loved this NLP course! It's very informative and engaging.\"\n",
    "# cleaned = spacy_preprocess(text)\n",
    "# print(cleaned)\n",
    "# 📤 Output (approx):\n",
    "\n",
    "# nginx\n",
    "# Copy\n",
    "# Edit\n",
    "# absolutely love nlp course informative engaging\n",
    "# 🔁 On Your Dataset:\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# dataset['Cleaned_Review'] = dataset['Review'].apply(spacy_preprocess)\n",
    "# Then vectorize:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizer = CountVectorizer(max_features=1500)\n",
    "# X = vectorizer.fit_transform(dataset['Cleaned_Review']).toarray()\n",
    "# ⚖️ spaCy vs NLTK – Quick Summary\n",
    "# Feature\tNLTK\tspaCy\n",
    "# Tokenization\tManual\tBuilt-in and robust\n",
    "# Lemmatization\tGood (WordNet)\tVery Good and faster\n",
    "# POS/NER\tBasic\tAdvanced\n",
    "# Speed\tSlower\tFaster\n",
    "# Learning Curve\tBeginner-friendly\tMore Pythonic and modern\n",
    "# Use Case\tTeaching, research\tProduction, commercial NLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
